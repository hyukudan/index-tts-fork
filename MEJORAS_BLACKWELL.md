# Resumen de Mejoras para GPUs Modernas (Blackwell, Ada, etc.)

## üéØ Objetivo

Hacer que IndexTTS funcione perfectamente en GPUs modernas como **RTX 6000 Blackwell**, resolviendo problemas con Wav2Vec2Bert, Whisper, Flash Attention, y a√±adiendo soporte multi-GPU.

---

## ‚úÖ Mejoras Implementadas

### 1. üéÆ Sistema Multi-GPU con Selecci√≥n Interactiva

**Problema anterior:**
- Solo usaba GPU 0 siempre
- No hab√≠a forma de elegir GPU en sistemas multi-GPU
- No se detectaba la plataforma (WSL vs Windows vs Linux)

**Soluci√≥n implementada:**

Nuevo m√≥dulo: `indextts/utils/gpu_config.py`

**Primera ejecuci√≥n:**
```bash
python webui.py
```

```
üöÄ IndexTTS GPU Configuration
==================================================
üìç Platform: WSL2 on Windows
   üí° WSL2 often provides better performance than Windows native

üéÆ Detected 2 GPU(s):

  [0] NVIDIA GeForce RTX 4090
      Architecture: Ada Lovelace (sm_8.9)
      Memory: 24.0 GB
      Suggested workers: 3
      ‚ú® Ada Lovelace GPU - excellent performance
         ‚Ä¢ Flash Attention available via pip

  [1] NVIDIA RTX 6000 Ada Generation
      Architecture: Blackwell (sm_10.0)
      Memory: 48.0 GB
      Suggested workers: 6
      üíé Blackwell GPU detected!
         ‚Ä¢ BF16 recommended for stability
         ‚Ä¢ Flash Attention: build from source required

‚ö° Flash Attention: Not installed
   Install with: uv sync --extra flashattn
   ‚ö†Ô∏è  Blackwell detected: Build from source required!
      See INSTALLATION_UPDATED.md for instructions

üéØ Select GPU to use [0-1]: 1

‚úÖ Configuration saved to: ~/.indextts/gpu_config.json
   Selected GPU: NVIDIA RTX 6000 Ada Generation
```

**Caracter√≠sticas:**
- ‚úÖ Detecta TODAS las GPUs disponibles
- ‚úÖ Muestra arquitectura, memoria, compute capability
- ‚úÖ Sugerencia autom√°tica de workers seg√∫n VRAM
- ‚úÖ Detecta Blackwell, Ada, Ampere, Turing, Volta
- ‚úÖ Detecta WSL2 vs Windows vs Linux
- ‚úÖ Guarda configuraci√≥n para pr√≥ximas ejecuciones
- ‚úÖ Argumento `--gpu N` para override
- ‚úÖ Detecci√≥n de Flash Attention

**Uso:**
```bash
# Primera vez: selecci√≥n interactiva
python webui.py

# Forzar GPU espec√≠fica
python webui.py --gpu 1

# Reconfigurar
rm ~/.indextts/gpu_config.json
python webui.py
```

---

### 2. üõ°Ô∏è Manejo de Errores OOM (Out of Memory)

**Problema anterior:**
- Crashes sin mensajes claros
- No se limpiaba memoria CUDA
- Dif√≠cil saber qu√© ajustar

**Soluci√≥n implementada:**

**En `webui.py` y `webui_parallel.py`:**
- Try-catch espec√≠fico para RuntimeError OOM
- Limpieza autom√°tica de cache CUDA
- Mensajes claros con sugerencias

**Ejemplo de error mejorado:**
```
‚ö†Ô∏è GPU out of memory. Try reducing max_mel_tokens, max_text_tokens_per_sentence, or duration.
   Error: CUDA out of memory. Tried to allocate 2.50 GiB...
```

**D√≥nde se aplica:**
- `gen_single()` - Generaci√≥n individual
- `generate_all_batch()` - Generaci√≥n batch
- `regenerate_batch_entry()` - Regeneraci√≥n
- `_worker_loop()` - Workers paralelos

**Funci√≥n de limpieza:**
```python
def cleanup_gpu_memory():
    """Clean up GPU memory to prevent OOM errors."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()
```

Se llama autom√°ticamente despu√©s de cada batch.

---

### 3. üîß Actualizaci√≥n de Dependencias

**Problema anterior:**
- `flash-attn` hardcoded wheel solo para Windows Python 3.10
- Versiones pinned imped√≠an actualizaciones
- No funcionaba en Linux ni Blackwell

**Cambios en `pyproject.toml`:**

```diff
- "accelerate==1.8.1"
+ "accelerate>=1.8.1"  # Permite updates

- "numpy==1.26.2"
+ "numpy>=1.26.2,<2.0"  # Flexibilidad sin romper

- "transformers==4.52.1"
+ "transformers>=4.52.1"  # Fixes CUDA 12.8

- "safetensors==0.5.2"
+ "safetensors>=0.5.2"

- "tokenizers==0.21.0"
+ "tokenizers>=0.21.0"

- flash-attn = { path = "flash_attn-...-win_amd64.whl" }
+ # Movido a [project.optional-dependencies]
```

**Nuevo extra opcional:**
```toml
[project.optional-dependencies]
flashattn = [
  "flash-attn>=2.8.0; sys_platform == 'linux'",
]
```

**Instalaci√≥n:**
```bash
# Sin Flash Attention (funciona, m√°s lento)
uv sync

# Con Flash Attention (RTX 4090, 3090)
uv sync --extra flashattn

# Blackwell - build desde fuente
uv sync
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention
MAX_JOBS=4 FLASH_ATTENTION_FORCE_BUILD=TRUE TORCH_CUDA_ARCH_LIST="10.0" python setup.py install
```

---

### 4. üö® Detecci√≥n y Warnings Espec√≠ficos

**Nuevos checks al inicio:**

1. **Arquitectura GPU**
   - Blackwell (sm_10.0+): Tip sobre BF16
   - Ada (sm_8.9): Info sobre Flash Attention
   - Ampere/Turing/Volta: Identificaci√≥n

2. **Flash Attention**
   - Detecta si est√° instalado
   - Muestra versi√≥n
   - Warning si no est√° (con instrucciones)

3. **Plataforma**
   - Detecta WSL2 vs Windows vs Linux
   - Recomienda WSL2 si est√°s en Windows nativo

4. **Memoria GPU**
   - Muestra VRAM total
   - Calcula workers sugeridos

5. **Optimizaciones CUDA**
   - Para Blackwell: activa `CUDA_LAUNCH_BLOCKING=0` y `TORCH_CUDNN_V8_API_ENABLED=1`

---

### 5. üìö Documentaci√≥n Completa

#### `GPU_TROUBLESHOOTING.md`
- **Problemas comunes** con Wav2Vec2Bert, Whisper, transformers
- **Soluciones** para Flash Attention en Blackwell
- **Problemas FP16 vs BF16** en Blackwell
- **Errores OOM** y c√≥mo resolverlos
- **Recomendaciones** por modelo de GPU
- **Comandos de debugging**
- **Secci√≥n multi-GPU** completa

#### `INSTALLATION_UPDATED.md`
- **Gu√≠a de instalaci√≥n** actualizada
- **Opciones de instalaci√≥n** por GPU
- **Ejemplos** de selecci√≥n interactiva
- **Troubleshooting** com√∫n
- **Scripts de verificaci√≥n**
- **Recomendaciones** por GPU
- **Migraci√≥n** desde versi√≥n anterior

#### `MEJORAS_BLACKWELL.md` (este archivo)
- Resumen ejecutivo de todas las mejoras
- Ejemplos de uso
- Casos de uso
- Checklist de verificaci√≥n

---

## üéØ Casos de Uso Resueltos

### Caso 1: Sistema con Blackwell + Ada

**Situaci√≥n:**
- PC con RTX 6000 Blackwell (48GB) + RTX 4090 (24GB)
- Quieres usar Blackwell para producci√≥n, Ada para desarrollo

**Soluci√≥n:**
```bash
# Producci√≥n en Blackwell
python webui_parallel.py --gpu 0

# Desarrollo en Ada
python webui.py --gpu 1
```

### Caso 2: Errores con Wav2Vec2Bert en Blackwell

**S√≠ntomas:**
- NaN en outputs
- CUDA errors
- Crashes durante inferencia

**Causas identificadas:**
1. FP16 inestable en Blackwell
2. Flash Attention wheel incompatible
3. transformers versi√≥n vieja

**Soluci√≥n:**
1. Actualizar dependencias: `uv sync`
2. Considerar NO usar `--is_fp16` (o usar BF16 si se implementa)
3. Build Flash Attention desde fuente
4. Ver `GPU_TROUBLESHOOTING.md` secci√≥n "Wav2Vec2Bert"

### Caso 3: WSL vs Windows Nativo

**Problema:**
- Rendimiento diferente
- Compatibilidad de Flash Attention

**Soluci√≥n:**
- Sistema detecta autom√°ticamente WSL
- Recomienda WSL2 si est√°s en Windows
- Flash Attention funciona mejor en WSL2

---

## üìä Verificaci√≥n de Mejoras

### Checklist de Verificaci√≥n

```bash
# 1. Verificar instalaci√≥n
uv sync

# 2. Primera ejecuci√≥n - configuraci√≥n GPU
python webui.py

# Deber√≠as ver:
# ‚úÖ Selecci√≥n interactiva de GPU
# ‚úÖ Detecci√≥n de plataforma
# ‚úÖ Info de todas las GPUs
# ‚úÖ Flash Attention status
# ‚úÖ Recomendaciones espec√≠ficas

# 3. Verificar config guardada
cat ~/.indextts/gpu_config.json

# 4. Verificar argumento --gpu
python webui.py --gpu 0

# 5. Verificar manejo de OOM
# Intenta generar con par√°metros muy altos
# Deber√≠as ver mensaje claro de OOM con sugerencias

# 6. Verificar multi-GPU
nvidia-smi  # Ver que usa la GPU correcta
```

### Script de Verificaci√≥n

```python
# Guarda como check_mejoras.py
import torch
from indextts.utils.gpu_config import GPUConfig

config = GPUConfig()

print("=== Verificaci√≥n de Mejoras ===\n")

# 1. Detecci√≥n de plataforma
platform_info = config.detect_platform()
print(f"1. Plataforma: {'WSL2' if platform_info['is_wsl'] else platform_info['system']}")

# 2. GPUs detectadas
gpus = config.get_gpu_info()
print(f"2. GPUs detectadas: {len(gpus)}")
for gpu in gpus:
    print(f"   [{gpu['id']}] {gpu['name']} - {gpu['architecture']}")

# 3. Flash Attention
flash_info = config.check_flash_attention()
print(f"3. Flash Attention: {'Instalado v' + flash_info['version'] if flash_info['installed'] else 'No instalado'}")

# 4. Config guardada
saved_gpu = config.config.get("selected_gpu_id")
print(f"4. GPU guardada: {saved_gpu if saved_gpu is not None else 'Ninguna (primera ejecuci√≥n)'}")

# 5. CUDA disponible
print(f"5. CUDA disponible: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   CUDA version: {torch.version.cuda}")
    print(f"   PyTorch: {torch.__version__}")

print("\n‚úÖ Verificaci√≥n completa!")
```

Ejecutar:
```bash
python check_mejoras.py
```

---

## üîÑ Flujo de Trabajo Recomendado

### Para desarrollo en sistema multi-GPU:

```bash
# 1. Primera ejecuci√≥n - configura GPU favorita
python webui.py
# Selecciona tu GPU de desarrollo

# 2. Desarrollo normal
python webui.py
# Usa GPU guardada autom√°ticamente

# 3. Testear en otra GPU
python webui.py --gpu 1
# Override temporal

# 4. Cambiar GPU permanente
rm ~/.indextts/gpu_config.json
python webui.py
# Reconfigurar
```

### Para sistemas con Blackwell:

```bash
# 1. Instalar dependencias base
uv sync

# 2. Build Flash Attention desde fuente
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention
MAX_JOBS=4 FLASH_ATTENTION_FORCE_BUILD=TRUE TORCH_CUDA_ARCH_LIST="10.0" python setup.py install
cd ..

# 3. Primera ejecuci√≥n
python webui.py
# Ver√°s recomendaciones espec√≠ficas para Blackwell

# 4. Monitorear memoria durante uso
watch -n 1 nvidia-smi

# 5. Si hay OOM, ajustar par√°metros seg√∫n mensajes
# El sistema sugiere qu√© reducir
```

---

## üìà Comparaci√≥n Antes vs Despu√©s

| Aspecto | Antes | Despu√©s |
|---------|-------|---------|
| **Multi-GPU** | Solo GPU 0 | Selecci√≥n interactiva |
| **Detecci√≥n plataforma** | No | WSL/Windows/Linux |
| **Errores OOM** | Crash sin info | Mensaje claro + sugerencias |
| **Flash Attention** | Wheel hardcoded | Opcional + build instructions |
| **Dependencias** | Versiones fijas | Actualizables dentro de constraints |
| **Blackwell** | Problemas de compatibilidad | Detectado con recomendaciones |
| **Configuraci√≥n** | Manual en c√≥digo | Interactiva + persistente |
| **Documentaci√≥n** | B√°sica | Completa con troubleshooting |

---

## üöÄ Pr√≥ximos Pasos Recomendados

### Mejoras futuras posibles:

1. **Soporte BF16 expl√≠cito**
   - Agregar opci√≥n `--use_bf16` para Blackwell
   - Conversi√≥n autom√°tica de modelos a BF16

2. **Telemetr√≠a de GPU**
   - Logging de uso de VRAM durante inferencia
   - Alertas proactivas antes de OOM

3. **Perfiles de configuraci√≥n**
   - Perfiles predefinidos por GPU
   - "Blackwell optimized", "Ada balanced", etc.

4. **Benchmark autom√°tico**
   - Testear rendimiento en primera ejecuci√≥n
   - Sugerir par√°metros √≥ptimos

5. **Multi-GPU paralelo**
   - Distribuir batch entre m√∫ltiples GPUs
   - Balanceo de carga autom√°tico

---

## üìû Soporte

### Si encuentras problemas:

1. **Revisa la documentaci√≥n:**
   - `GPU_TROUBLESHOOTING.md` - Problemas comunes
   - `INSTALLATION_UPDATED.md` - Instalaci√≥n paso a paso

2. **Ejecuta verificaci√≥n:**
   ```bash
   python check_mejoras.py
   ```

3. **Check verbose:**
   ```bash
   python webui.py --verbose
   ```

4. **Info de sistema:**
   ```bash
   nvidia-smi
   python -c "import torch; print(torch.__version__, torch.version.cuda)"
   ```

5. **Reset configuraci√≥n:**
   ```bash
   rm ~/.indextts/gpu_config.json
   python webui.py
   ```

---

## ‚ú® Cr√©ditos

Estas mejoras resuelven problemas espec√≠ficos identificados en sistemas con:
- RTX 6000 Blackwell
- RTX 4090 Ada Lovelace
- WSL2 en Windows
- Linux con CUDA 12.8
- Sistemas multi-GPU

Todas las mejoras son compatibles hacia atr√°s con GPUs m√°s antiguas (Ampere, Turing, Volta).

---

**√öltima actualizaci√≥n:** 2025-01-18
**Versi√≥n:** 2.0 con soporte multi-GPU y Blackwell

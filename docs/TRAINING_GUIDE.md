# GuÃ­a de Entrenamiento y Fine-tuning - IndexTTS-2

Esta guÃ­a explica cÃ³mo entrenar o hacer fine-tuning de modelos IndexTTS-2 para nuevos idiomas **sin sobrescribir los modelos base**.

## ğŸ¯ Objetivo: Preservar Modelos Base

Cuando entrenas para un nuevo idioma (ej: catalÃ¡n, francÃ©s, etc.), **NUNCA** sobrescribirÃ¡s los modelos originales en `checkpoints/`. En su lugar:

1. EntrenarÃ¡s en un directorio separado
2. CopiarÃ¡s el mejor modelo con un nombre descriptivo
3. El sistema de auto-detecciÃ³n lo encontrarÃ¡ automÃ¡ticamente

## ğŸ“‚ Estructura de Directorios Recomendada

```
index-tts-fork/
â”œâ”€â”€ checkpoints/                          # Modelos base (NO MODIFICAR)
â”‚   â”œâ”€â”€ gpt.pth                          # â† Modelo base original
â”‚   â”œâ”€â”€ bpe.model                        # â† Tokenizer base original
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ training/                             # Directorio de entrenamiento
â”‚   â”œâ”€â”€ catalan/                         # â† Entrenamiento catalÃ¡n
â”‚   â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â”‚   â”œâ”€â”€ catalan_bpe.model        # Tokenizer catalÃ¡n
â”‚   â”‚   â”‚   â””â”€â”€ catalan_bpe.vocab
â”‚   â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”‚   â”œâ”€â”€ model_step1000.pth       # Checkpoints intermedios
â”‚   â”‚   â”‚   â”œâ”€â”€ model_step2000.pth
â”‚   â”‚   â”‚   â””â”€â”€ latest.pth               # Ãšltimo checkpoint
â”‚   â”‚   â””â”€â”€ logs/                        # TensorBoard logs
â”‚   â”‚
â”‚   â”œâ”€â”€ french/                          # â† Entrenamiento francÃ©s
â”‚   â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â”‚   â”œâ”€â”€ french_bpe.model
â”‚   â”‚   â”‚   â””â”€â”€ french_bpe.vocab
â”‚   â”‚   â””â”€â”€ checkpoints/
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ multilingual/                    # â† Multilingual
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ models/                               # Modelos finalizados (para uso)
    â”œâ”€â”€ gpt_catalan.pth                  # â† Copia del mejor checkpoint
    â”œâ”€â”€ catalan_bpe.model                # â† Tokenizer catalÃ¡n
    â”œâ”€â”€ gpt_french.pth
    â”œâ”€â”€ french_bpe.model
    â””â”€â”€ ...
```

## ğŸ”§ Paso 1: Entrenar Tokenizer BPE

Para un nuevo idioma, primero crea un tokenizer especÃ­fico:

```bash
# Ejemplo: Tokenizer para catalÃ¡n
python tools/tokenizer/train_bpe.py \
    --manifest data/catalan_dataset/train.jsonl \
    --output-prefix training/catalan/tokenizer/catalan_bpe \
    --vocab-size 12000 \
    --character-coverage 0.9995
```

**Resultado:**
- `training/catalan/tokenizer/catalan_bpe.model` âœ“
- `training/catalan/tokenizer/catalan_bpe.vocab` âœ“

**âœ… El modelo base `checkpoints/bpe.model` NO se toca**

## ğŸš€ Paso 2: Fine-tune del Modelo GPT

Entrena el modelo usando el tokenizer especÃ­fico:

```bash
# Ejemplo: Fine-tuning para catalÃ¡n
python trainers/train_gpt_v2.py \
    --train-manifest data/catalan_dataset/train_paired.jsonl \
    --val-manifest data/catalan_dataset/val_paired.jsonl \
    --tokenizer training/catalan/tokenizer/catalan_bpe.model \
    --config checkpoints/config.yaml \
    --base-checkpoint checkpoints/gpt.pth \
    --output-dir training/catalan/checkpoints \
    --batch-size 4 \
    --epochs 10 \
    --learning-rate 2e-5
```

**Argumentos clave:**
- `--tokenizer`: Tu tokenizer especÃ­fico (NO el base)
- `--base-checkpoint`: Modelo base para fine-tuning
- `--output-dir`: Directorio separado (NO `checkpoints/`)

**Resultado:**
- Modelos en `training/catalan/checkpoints/model_step*.pth`
- Logs en `training/catalan/checkpoints/runs/`

**âœ… El modelo base `checkpoints/gpt.pth` NO se modifica**

## ğŸ“¦ Paso 3: Finalizar y Publicar Modelo

Una vez tengas el mejor checkpoint (ej: `model_step5000.pth`), usa el script helper:

```bash
# Instalar modelo finalizado para uso en WebUI
python tools/install_trained_model.py \
    --checkpoint training/catalan/checkpoints/model_step5000.pth \
    --tokenizer training/catalan/tokenizer/catalan_bpe.model \
    --output-name catalan \
    --description "Catalan fine-tuned model"
```

Este script:
1. Copia el checkpoint a `models/gpt_catalan.pth`
2. Copia el tokenizer a `models/catalan_bpe.model`
3. Actualiza el registro de modelos

**Alternativamente (manual):**

```bash
# Crear directorio de modelos
mkdir -p models

# Copiar checkpoint con nombre descriptivo
cp training/catalan/checkpoints/model_step5000.pth models/gpt_catalan.pth

# Copiar tokenizer con nombre especÃ­fico
cp training/catalan/tokenizer/catalan_bpe.model models/catalan_bpe.model
```

## ğŸ¨ ConvenciÃ³n de Nombres

Para que el sistema de auto-detecciÃ³n funcione correctamente:

### âœ… OpciÃ³n 1: Mismo nombre base
```
models/
â”œâ”€â”€ gpt_catalan.pth
â””â”€â”€ gpt_catalan_bpe.model  â† Auto-detectado por coincidencia de nombre
```

### âœ… OpciÃ³n 2: Nombre estÃ¡ndar en mismo directorio
```
models/
â”œâ”€â”€ gpt_catalan.pth
â””â”€â”€ catalan_bpe.model     â† TambiÃ©n funciona si contiene "catalan"
```

### âœ… OpciÃ³n 3: Subdirectorio tokenizers
```
models/
â”œâ”€â”€ gpt_catalan.pth
â””â”€â”€ tokenizers/
    â””â”€â”€ catalan_bpe.model  â† Auto-detectado en subdirectorio
```

### âŒ NO HAGAS ESTO:
```
checkpoints/
â”œâ”€â”€ gpt.pth              â† SOBRESCRITO (MAL!)
â””â”€â”€ bpe.model            â† PERDIDO (MAL!)
```

## ğŸ–¥ï¸ Uso en WebUI

DespuÃ©s de instalar el modelo, aparecerÃ¡ automÃ¡ticamente en la WebUI:

1. **Dropdown "Model Checkpoint":**
   ```
   gpt.pth (3.2GB, v2.0, zh/en)           â† Original
   gpt_catalan.pth (3.2GB, v2.0, ca)      â† Tu modelo
   gpt_french.pth (3.2GB, v2.0, fr)       â† Otro modelo
   ```

2. **Metadata auto-detectada:**
   ```
   Tokenizer: catalan_bpe.model (12000 vocab)
   VRAM: 8.1 GB
   ```

3. **GPU selector:** Elige GPU 0 o GPU 1

4. **Load Model:** Carga con hot-swap

## ğŸ”„ Comparar Modelos

En la pestaÃ±a "Compare Models":

```
Model A: gpt.pth (original)
Model B: gpt_catalan.pth (catalÃ¡n)

â†’ Genera con ambos
â†’ Compara RTF, calidad, mÃ©tricas
```

## ğŸ“‹ Ejemplo Completo: AÃ±adir CatalÃ¡n

```bash
# 1. Preparar datos (ver tools/prepare_dataset.py)
python tools/prepare_dataset.py \
    --audio-dir data/raw/catalan_audio/ \
    --transcript-file data/raw/catalan_transcripts.txt \
    --output-manifest data/catalan_dataset/train.jsonl

# 2. Crear pares prompt/target para GPT
python tools/build_gpt_prompt_pairs.py \
    --manifest data/catalan_dataset/train.jsonl \
    --output data/catalan_dataset/train_paired.jsonl

# 3. Entrenar tokenizer
python tools/tokenizer/train_bpe.py \
    --manifest data/catalan_dataset/train.jsonl \
    --output-prefix training/catalan/tokenizer/catalan_bpe \
    --vocab-size 12000

# 4. Fine-tune modelo
python trainers/train_gpt_v2.py \
    --train-manifest data/catalan_dataset/train_paired.jsonl \
    --val-manifest data/catalan_dataset/val_paired.jsonl \
    --tokenizer training/catalan/tokenizer/catalan_bpe.model \
    --base-checkpoint checkpoints/gpt.pth \
    --output-dir training/catalan/checkpoints \
    --epochs 10

# 5. Instalar mejor checkpoint (ej: step 5000)
python tools/install_trained_model.py \
    --checkpoint training/catalan/checkpoints/model_step5000.pth \
    --tokenizer training/catalan/tokenizer/catalan_bpe.model \
    --output-name catalan

# 6. Usar en WebUI
python webui.py --model-dir models
```

## âš ï¸ Checklist de Seguridad

Antes de entrenar, verifica:

- [ ] `--output-dir` NO es `checkpoints/`
- [ ] `--tokenizer` apunta a TU tokenizer, no al base
- [ ] Tienes backup de `checkpoints/` original
- [ ] El nombre del modelo final incluye el idioma (ej: `gpt_catalan.pth`)
- [ ] El tokenizer final tiene nombre relacionado (ej: `catalan_bpe.model`)

## ğŸ“ Tips Avanzados

### MÃºltiples idiomas en un modelo

```bash
# Combinar datasets
python trainers/train_gpt_v2.py \
    --train-manifest data/catalan/train.jsonl::ca \
    --train-manifest data/spanish/train.jsonl::es \
    --train-manifest data/french/train.jsonl::fr \
    --output-dir training/multilingual_cat_es_fr \
    --output-name multilingual_romance
```

### Continuar entrenamiento

```bash
python trainers/train_gpt_v2.py \
    --resume training/catalan/checkpoints/latest.pth \
    --epochs 20  # Continuar mÃ¡s Ã©pocas
```

### Usar GPU especÃ­fica

```bash
CUDA_VISIBLE_DEVICES=1 python trainers/train_gpt_v2.py \
    --output-dir training/catalan/checkpoints \
    ...
```

## ğŸ“š Recursos Adicionales

- **PreparaciÃ³n de datos:** `tools/README.md`
- **Tokenizer custom:** `tools/tokenizer/README.md`
- **Training avanzado:** `trainers/README.md`
- **WebUI features:** `docs/WEBUI_GUIDE.md`

---

**Regla de Oro:** Nunca modifiques archivos en `checkpoints/` directamente. Usa directorios separados y copia solo cuando estÃ©s seguro del resultado final.
